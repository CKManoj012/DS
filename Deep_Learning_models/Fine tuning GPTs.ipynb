{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddeabedc-ab55-49bf-bc36-c5e3abb1dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7397e8e-be49-458a-b1b5-c9a6feeda5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (adjust the filename as needed)\n",
    "data = pd.read_csv(\"imdb_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bd9d38-9a77-4628-8ec8-2330b549d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'sentiment'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Check the dataset\n",
    "print(hf_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d593caea-69c0-4796-a92f-98493539f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d58f1d-7bbe-45c5-8d5a-54e10ba951c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d396023fff46e7b037c1b4f0074ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manoj\\anaconda3\\envs\\port\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manoj\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aa6fb420944a5caa7e6c612afcda5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998746b902684e5db090af0c3abbc00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd710182097498bb141fc6d907b6959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27572543f2e7434198b9397e3bea3fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading GPT-2 tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab6b363b-ae3b-4011-8b10-c9c969fc4f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef780f02f5645348d25b1b711f3cd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define tokenizing function\n",
    "# Set pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['review'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()  # Set labels as a copy of input_ids\n",
    "    return tokenized\n",
    "## Applying tokeinization\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c170d6-1e86-40ea-b09d-39886b70a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting format for pytorch\n",
    "\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5b0f1e-ed55-4346-90b0-4874cd1dd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset to train and test\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2,seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc4d1ef-5866-43c9-b5c5-f648d3e4cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Training Arguments\n",
    "\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7207d7c-2f18-4ed8-a03a-e137f4657d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11509503-0f8f-4bf3-938a-9e73e1d419f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8b1c488-0b71-467f-b46d-8ae55763dc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 2:11:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.547000</td>\n",
       "      <td>3.530348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.065400</td>\n",
       "      <td>3.552714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.188100</td>\n",
       "      <td>3.577069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "4.109\n",
      "Attempted to log scalar metric grad_norm:\n",
      "10.864067077636719\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.9166666666666665e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.05\n",
      "Attempted to log scalar metric loss:\n",
      "3.7817\n",
      "Attempted to log scalar metric grad_norm:\n",
      "10.540285110473633\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.8333333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.1\n",
      "Attempted to log scalar metric loss:\n",
      "3.7492\n",
      "Attempted to log scalar metric grad_norm:\n",
      "10.254212379455566\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.75e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.15\n",
      "Attempted to log scalar metric loss:\n",
      "3.8009\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.807520866394043\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.666666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.2\n",
      "Attempted to log scalar metric loss:\n",
      "3.739\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.99795150756836\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.5833333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.25\n",
      "Attempted to log scalar metric loss:\n",
      "3.6037\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.502053260803223\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.3\n",
      "Attempted to log scalar metric loss:\n",
      "3.6851\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.491989135742188\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.4166666666666665e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.35\n",
      "Attempted to log scalar metric loss:\n",
      "3.5777\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.877989768981934\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.3333333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.4\n",
      "Attempted to log scalar metric loss:\n",
      "3.6849\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.803028106689453\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.25e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.45\n",
      "Attempted to log scalar metric loss:\n",
      "3.7575\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.707843780517578\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.166666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.5\n",
      "Attempted to log scalar metric loss:\n",
      "3.8047\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.124320030212402\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.0833333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.55\n",
      "Attempted to log scalar metric loss:\n",
      "3.8281\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.063934326171875\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.6\n",
      "Attempted to log scalar metric loss:\n",
      "3.7892\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.542703628540039\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.9166666666666665e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.65\n",
      "Attempted to log scalar metric loss:\n",
      "3.6382\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.4863691329956055\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.8333333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.7\n",
      "Attempted to log scalar metric loss:\n",
      "3.7745\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.008002281188965\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.7500000000000003e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.75\n",
      "Attempted to log scalar metric loss:\n",
      "3.7631\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.815481662750244\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.6666666666666666e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.8\n",
      "Attempted to log scalar metric loss:\n",
      "3.5741\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.360508441925049\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.5833333333333335e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.85\n",
      "Attempted to log scalar metric loss:\n",
      "3.606\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.837552070617676\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.9\n",
      "Attempted to log scalar metric loss:\n",
      "3.6564\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.130201816558838\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.4166666666666666e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "0.95\n",
      "Attempted to log scalar metric loss:\n",
      "3.547\n",
      "Attempted to log scalar metric grad_norm:\n",
      "5.853058815002441\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.3333333333333335e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "Attempted to log scalar metric eval_loss:\n",
      "3.5303475856781006\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "139.9065\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "1.43\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.357\n",
      "Attempted to log scalar metric epoch:\n",
      "1.0\n",
      "Attempted to log scalar metric loss:\n",
      "3.4105\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.6359639167785645\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.2500000000000004e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.05\n",
      "Attempted to log scalar metric loss:\n",
      "3.4247\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.570756435394287\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.1666666666666666e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.1\n",
      "Attempted to log scalar metric loss:\n",
      "3.2969\n",
      "Attempted to log scalar metric grad_norm:\n",
      "6.576444149017334\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.0833333333333335e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.15\n",
      "Attempted to log scalar metric loss:\n",
      "3.5241\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.513131618499756\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.2\n",
      "Attempted to log scalar metric loss:\n",
      "3.2891\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.053668022155762\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.916666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.25\n",
      "Attempted to log scalar metric loss:\n",
      "3.4961\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.841303825378418\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.8333333333333335e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.3\n",
      "Attempted to log scalar metric loss:\n",
      "3.2472\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.013318061828613\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.7500000000000004e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.35\n",
      "Attempted to log scalar metric loss:\n",
      "3.3791\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.39976978302002\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.6666666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.4\n",
      "Attempted to log scalar metric loss:\n",
      "3.3179\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.422791481018066\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5833333333333336e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.45\n",
      "Attempted to log scalar metric loss:\n",
      "3.3938\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.512154579162598\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.5\n",
      "Attempted to log scalar metric loss:\n",
      "3.3674\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.219958305358887\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.4166666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.55\n",
      "Attempted to log scalar metric loss:\n",
      "3.5265\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.870494842529297\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.3333333333333336e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.6\n",
      "Attempted to log scalar metric loss:\n",
      "3.3406\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.178187370300293\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.25e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.65\n",
      "Attempted to log scalar metric loss:\n",
      "3.3803\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.80126953125\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.1666666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.7\n",
      "Attempted to log scalar metric loss:\n",
      "3.4189\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.688938140869141\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.0833333333333336e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.75\n",
      "Attempted to log scalar metric loss:\n",
      "3.4574\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.336922645568848\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.8\n",
      "Attempted to log scalar metric loss:\n",
      "3.4897\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.840023040771484\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.9166666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.85\n",
      "Attempted to log scalar metric loss:\n",
      "3.3838\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.28654670715332\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.8333333333333333e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.9\n",
      "Attempted to log scalar metric loss:\n",
      "3.4123\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.340826034545898\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.75e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "1.95\n",
      "Attempted to log scalar metric loss:\n",
      "3.0654\n",
      "Attempted to log scalar metric grad_norm:\n",
      "7.8540239334106445\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.6666666666666667e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "Attempted to log scalar metric eval_loss:\n",
      "3.552713632583618\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "140.1063\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "1.427\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.357\n",
      "Attempted to log scalar metric epoch:\n",
      "2.0\n",
      "Attempted to log scalar metric loss:\n",
      "3.3079\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.517773628234863\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.5833333333333333e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.05\n",
      "Attempted to log scalar metric loss:\n",
      "3.2174\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.718886375427246\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.5e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.1\n",
      "Attempted to log scalar metric loss:\n",
      "2.9817\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.568775177001953\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.4166666666666668e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.15\n",
      "Attempted to log scalar metric loss:\n",
      "3.3545\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.63659381866455\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.3333333333333333e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.2\n",
      "Attempted to log scalar metric loss:\n",
      "3.2323\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.782958030700684\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.25e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.25\n",
      "Attempted to log scalar metric loss:\n",
      "3.3645\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.077622413635254\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.1666666666666668e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.3\n",
      "Attempted to log scalar metric loss:\n",
      "3.21\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.079814910888672\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.0833333333333334e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.35\n",
      "Attempted to log scalar metric loss:\n",
      "3.1754\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.299232482910156\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1e-05\n",
      "Attempted to log scalar metric epoch:\n",
      "2.4\n",
      "Attempted to log scalar metric loss:\n",
      "3.2605\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.427864074707031\n",
      "Attempted to log scalar metric learning_rate:\n",
      "9.166666666666666e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.45\n",
      "Attempted to log scalar metric loss:\n",
      "3.2585\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.386192321777344\n",
      "Attempted to log scalar metric learning_rate:\n",
      "8.333333333333334e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.5\n",
      "Attempted to log scalar metric loss:\n",
      "3.219\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.018589973449707\n",
      "Attempted to log scalar metric learning_rate:\n",
      "7.5e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.55\n",
      "Attempted to log scalar metric loss:\n",
      "3.216\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.225373268127441\n",
      "Attempted to log scalar metric learning_rate:\n",
      "6.666666666666667e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.6\n",
      "Attempted to log scalar metric loss:\n",
      "3.314\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.064202308654785\n",
      "Attempted to log scalar metric learning_rate:\n",
      "5.833333333333334e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.65\n",
      "Attempted to log scalar metric loss:\n",
      "3.2854\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.765819549560547\n",
      "Attempted to log scalar metric learning_rate:\n",
      "5e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.7\n",
      "Attempted to log scalar metric loss:\n",
      "3.0837\n",
      "Attempted to log scalar metric grad_norm:\n",
      "9.171764373779297\n",
      "Attempted to log scalar metric learning_rate:\n",
      "4.166666666666667e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.75\n",
      "Attempted to log scalar metric loss:\n",
      "3.2428\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.22375774383545\n",
      "Attempted to log scalar metric learning_rate:\n",
      "3.3333333333333333e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.8\n",
      "Attempted to log scalar metric loss:\n",
      "3.24\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.312928199768066\n",
      "Attempted to log scalar metric learning_rate:\n",
      "2.5e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.85\n",
      "Attempted to log scalar metric loss:\n",
      "3.1278\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.699374198913574\n",
      "Attempted to log scalar metric learning_rate:\n",
      "1.6666666666666667e-06\n",
      "Attempted to log scalar metric epoch:\n",
      "2.9\n",
      "Attempted to log scalar metric loss:\n",
      "3.1878\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.610516548156738\n",
      "Attempted to log scalar metric learning_rate:\n",
      "8.333333333333333e-07\n",
      "Attempted to log scalar metric epoch:\n",
      "2.95\n",
      "Attempted to log scalar metric loss:\n",
      "3.1881\n",
      "Attempted to log scalar metric grad_norm:\n",
      "8.01553726196289\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n",
      "Attempted to log scalar metric eval_loss:\n",
      "3.577069044113159\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "99.439\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "2.011\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "0.503\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n",
      "Attempted to log scalar metric train_runtime:\n",
      "7883.8345\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "0.304\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "0.076\n",
      "Attempted to log scalar metric total_flos:\n",
      "156775219200000.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "3.442647460301717\n",
      "Attempted to log scalar metric epoch:\n",
      "3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=3.442647460301717, metrics={'train_runtime': 7883.8345, 'train_samples_per_second': 0.304, 'train_steps_per_second': 0.076, 'total_flos': 156775219200000.0, 'train_loss': 3.442647460301717, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load GPT-2\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#Initialize the Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "## Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4fbbdd7-8052-421b-b4ba-3b603247cde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./imdb_gpt_model\\\\tokenizer_config.json',\n",
       " './imdb_gpt_model\\\\special_tokens_map.json',\n",
       " './imdb_gpt_model\\\\vocab.json',\n",
       " './imdb_gpt_model\\\\merges.txt',\n",
       " './imdb_gpt_model\\\\added_tokens.json',\n",
       " './imdb_gpt_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./imdb_gpt_model\")\n",
    "tokenizer.save_pretrained(\"./imdb_gpt_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5731c677-b3e3-4a91-858d-58c12c666e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"The movie was an interesting attempt at a different style; all three films were entertaining, but none really added any interesting plot and character arcs. The director simply felt that it wasn't quite as enjoyable as an enjoyable one, which is why I'd say\"}, {'generated_text': 'The movie was awful I guess. It didn\\'t have a lot going for it. I don\\'t know if I could feel pity for Mr. K. K. or if he was a nice person. I think he looked bored for example in \"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "generator = pipeline(\"text-generation\", model=\"./imdb_gpt_model\", tokenizer=tokenizer)\n",
    "\n",
    "# Test with a prompt\n",
    "result = generator(\"The movie was\", max_length=50, num_return_sequences=2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bfee99-7980-416e-88cf-7c5862ae0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation and testing\n",
    "\n",
    "\n",
    "\n",
    "new_review_raw = [\n",
    "\"Wow, this movie was just what I needed to cure my insomnia. Absolutely thrilling!\",\n",
    "\"Best movie ever! I loved wasting 3 hours of my life on this masterpiece.\",\n",
    "\"Oh sure, the acting was so 'natural' I almost believed the actors were wooden dolls.\",\n",
    "\"Definitely recommend this movie... if you want to bore yourself to death.\",\n",
    "\"The movie was about two friends who embark on a journey. It has a runtime of two hours.\",\n",
    "\"It is a typical superhero movie with action scenes and some emotional moments.\",\n",
    "\"The cinematography was colorful, and the soundtrack was loud.\",\n",
    "\"The second half of the movie was longer than the first.\",\n",
    "\"It was okay, I guess, but I wouldn’t watch it again.\",\n",
    "\"Not bad, but not great either.\",\n",
    "\"I laughed, I cried, but I still don’t know if I liked it or not.\",\n",
    "\"The second half was much better than the first, though the ending was questionable.\",\n",
    "\"It was very good, super, fantastic.\",\n",
    "\"It was good until the second half \",\n",
    "\"It was second half \",\n",
    "\"Second half was good \",\n",
    "\"Movie is amazing, especially in the Second half\",\n",
    "\"Terrible movie, Second half was hilarious\",\n",
    "\"It was okay, Second half was hilarious\",\n",
    "\"Best movie if you are looking for a headache\",\n",
    "\"Lots of fun\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74b971b-c855-421b-8150-f22759dd7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, this movie was just what I needed to cure my insomnia. Absolutely thrilling! I was hooked. I was hooked. I was hooked. I was hooked. I was hooked.\n",
      "Best movie ever! I loved wasting 3 hours of my life on this masterpiece. I was so disappointed. I was so disappointed in the movie. I was so disappointed in the acting\n",
      "Oh sure, the acting was so 'natural' I almost believed the actors were wooden dolls. But I was wrong. The acting was so 'natural' I almost believed the actors were wooden dolls\n",
      "Definitely recommend this movie... if you want to bore yourself to death. It's a very good movie. It's a very good movie. It's a very good movie\n",
      "The movie was about two friends who embark on a journey. It has a runtime of two hours. The first is a very good movie. The second is a very bad movie. The movie is a\n",
      "It is a typical superhero movie with action scenes and some emotional moments. The movie is a bit of a disappointment, but it is a good movie. The acting is good\n",
      "The cinematography was colorful, and the soundtrack was loud. The music was catchy, and the acting was good. The acting was good, and the acting was\n",
      "The second half of the movie was longer than the first. The first half was more of a story about a group of people who are trying to find a way\n",
      "It was okay, I guess, but I wouldn’t watch it again. I mean, I'm not a fan of the show, but I'm not a fan of the\n",
      "Not bad, but not great either. The story is a bit of a mystery, but the characters are believable and believable. The acting is\n",
      "I laughed, I cried, but I still don’t know if I liked it or not. I was so disappointed. I thought it was a great movie, but I didn't like it.\n",
      "The second half was much better than the first, though the ending was questionable. The first half was a bit more interesting, but the second half was much more predictable. The third\n",
      "It was very good, super, fantastic. I was surprised at how much I liked this movie. I was surprised at how much I liked the\n",
      "It was good until the second half  when I saw the trailer for the movie. I was so excited to see this movie. I\n",
      "It was second half  of the movie. The first half was a bit of a disappointment. The second half was a\n",
      "Second half was good !!! I was so excited to see this movie. I was so excited to see the movie that I\n",
      "Movie is amazing, especially in the Second half of the movie. The acting is great, the acting is great, the acting is great, the\n",
      "Terrible movie, Second half was hilarious, Third half was boring. I'm glad I didn't watch it. I'm glad I didn\n",
      "It was okay, Second half was hilarious, and the third half was a waste of time. I'm glad I didn't watch it.\n",
      "Best movie if you are looking for a headache. It is a very good movie. The acting is good and the acting is good. The acting\n",
      "Lots of fun to watch this movie. I was really surprised at how much of a plot this movie had. I\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load fine-tuned GPT-2 model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./imdb_gpt_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./imdb_gpt_model\")\n",
    "\n",
    "# Ensure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Initialize results DataFrame\n",
    "validationResults = pd.DataFrame()\n",
    "validationResults['Test cases'] = new_review_raw\n",
    "\n",
    "# Perform sentiment analysis\n",
    "predictions = []\n",
    "model.eval()\n",
    "for review in new_review_raw:\n",
    "    # Tokenize the input review\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,  # Control the number of tokens to generate\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Simple heuristic to classify sentiment\n",
    "    if \"positive\" in generated_text.lower():\n",
    "        predictions.append(\"Positive\")\n",
    "    elif \"negative\" in generated_text.lower():\n",
    "        predictions.append(\"Negative\")\n",
    "    else:\n",
    "        predictions.append(\"Neutral\")  # Fallback if sentiment is unclear\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "# validationResults['GPT-2 Prediction'] = predictions\n",
    "\n",
    "# # Display results\n",
    "# print(validationResults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ab67d-cbc3-4269-a7f0-3ae9d2bad6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad7fdd-42dc-4437-9231-771f52515ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fb1a5-bee2-41b5-a264-7605361a065b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c86bd-afa4-47c0-81df-d7d38b526b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5fdb2-9e80-470d-8544-6fffe0e3fd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164a2cb-1d95-4304-b4fe-0a346b8007bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
